## 加速神经网络训练

使用梯度下降的时候，有一个问题，就是在网络非常复杂的时候，梯度下降的时候对计算的需求非常高。

常见的几种加速求解cost的最小值的方法：
1. Stochastic Gradient Descent (SGD)

2. Momentum

3. AdaGrad

4. RMSProp

5. Adam


在tensorflow里面提供的几种优化器

1. tf.train.GradientDescentOptimizer
2. tf.train.AdaeltaOptimizer
3. tf.train.AdagradDAOptimizer
4. tf.train.AdamOptimizer
5. tf.train.MomentumOptimizer



参考文档 ： 

https://www.tensorflow.org/api_docs/python/tf/train
